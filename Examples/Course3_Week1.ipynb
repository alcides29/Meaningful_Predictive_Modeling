{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: MSE and R^2, Accuracy and Error, Precision and Recall\n",
    "\n",
    "#### Making Meaningful Predictions from Data\n",
    "This week we were introduced to three sets of concepts. We looked at MSE and R^2 along with their relation to one another. We learned about how to measure Accuracy and Error mathematically. And finally we learned about Precision and Recall using true/false positive/negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: MSE and R^2\n",
    "\n",
    "We are going to work with a 2D example of these topics for the purpose of visualization. In your capstone project at the end of this series you will use this on your dataset as a whole. This example is meant to be simpler as an introduction to the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The \"Data\"\n",
    "\n",
    "In this example we are going to use a 2D numpy array to calculate two separate MSE and R^2 values. The array defined below will be the \"data\" for this example.\n",
    "\n",
    "###### Note: Because of this exmaple using numpy arrays, most of our answers will return in an array form. Don't worry! This isn't affecting any of our answers! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linear_model.LinearRegression()\n",
    "exArray = np.array([[0], [1], [3], [2],\n",
    "                   [2],[3],[4], [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(exArray.shape) #Check the dimanesions of exArray using numpy operators \n",
    "#if you are unfamiliar with this don't worry it is just for this example\n",
    "\n",
    "exArray[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note that X and y must have the same dimensions!\n",
    "y = exArray[4:]\n",
    "X = exArray[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient: \n",
      " [[0.3]]\n"
     ]
    }
   ],
   "source": [
    "#We have to fit (train) our model!\n",
    "model.fit(X,y)\n",
    "# Theta would be the coef_\n",
    "print('Coefficient: \\n', model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The values of these test arrays are unimportant, so the values listed here are random.\n",
    "#Try changing them around and running through the below code again to see how they affect MSE and R^2\n",
    "\n",
    "# Original values for the example\n",
    "XTest = np.array([[-2]])\n",
    "yTest = np.array([[4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.2]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We predict the y1\n",
    "predictions = model.predict(XTest)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Calculate the squared differences as in lecture.\n",
    "differences = [(x-y)**2 for (x,y) in zip(predictions, y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.64])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE is just the average (Mean) of these squared differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = [0.64]\n"
     ]
    }
   ],
   "source": [
    "#TODO Calculate the MSE\n",
    "MSE = sum(differences)/len(differences)\n",
    "print(\"MSE = \" + str(MSE)) #Answer should be .64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the **Mean Absolute Error** would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE = 0.8000000000000003\n"
     ]
    }
   ],
   "source": [
    "MAE = np.mean([abs(x-y) for (x,y) in zip(predictions, y)])\n",
    "print(\"MAE = \" + str(MAE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in the lectures, the R^2 (and the FVU, or \"Fraction of Variance Unexplained\") normalize the Mean Squared Error based on the variance of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 = [0.70742857]\n"
     ]
    }
   ],
   "source": [
    "#TODO Calculate R2\n",
    "FVU = MSE / np.var(y)\n",
    "\n",
    "#SOLN (there are more ways than this to solve this)\n",
    "R2 = 1 - FVU\n",
    "\n",
    "print(\"R2 = \" + str(R2)) #Answer should be roughly .707..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Another example of MSE**  \n",
    "You can use any list below  \n",
    "a = actual list  \n",
    "p = predicted list\n",
    "\n",
    "The results should be:  \n",
    "MSE 1 = 8340.546666666667  \n",
    "MSE 2 = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8340.546666666667"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 21, 5, 7, 3.2, 4.2]\n",
    "p = [1, 222, 55, 77, 33, 41]\n",
    "\n",
    "#a = [4, 3, 2, 1]\n",
    "#p = [1, 2, 3, 4]\n",
    "\n",
    "def MSE(pred, actual):\n",
    "    dif = [(x-y)**2 for (x,y) in zip (p, a)]\n",
    "    return sum(dif)/len(dif)\n",
    "   \n",
    "MSE(p, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Accuracy and Error\n",
    "\n",
    "To start measuring accuracy and error, we must first replace our output variable with a binary variable indicating if the value in the array is greater than a select value. For this example we will check if the values are greater than or equal to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alcides/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([ True]), array([ True]), array([ True]), array([False])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_class = [(num >= 2) for num in y]\n",
    "modelA = linear_model.LogisticRegression()\n",
    "modelA.fit(X, y_class)\n",
    "y_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsA = modelA.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True],\n",
       "       [False, False, False, False]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = predictionsA == y_class\n",
    "correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Diagnostics: Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.75\n"
     ]
    }
   ],
   "source": [
    "#TODO Calculate Accuracy in any way not involving TP, TF, FP, FN\n",
    "\n",
    "accuracy = sum(correct)/len(predictionsA)\n",
    "print(\"Accuracy = \" + str(accuracy[0])) #Answer should be .75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = sum([(p and l) for (p,l) in zip(predictionsA, y_class)])\n",
    "FP = sum([(p and not l) for (p,l) in zip(predictionsA, y_class)])\n",
    "TN = sum([(not p and not l) for (p,l) in zip(predictionsA, y_class)])\n",
    "FN = sum([(not p and l) for (p,l) in zip(predictionsA, y_class)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP = [3]\n",
      "FP = 1\n",
      "TN = 0\n",
      "FN = 0\n"
     ]
    }
   ],
   "source": [
    "print(\"TP = \" + str(TP))\n",
    "print(\"FP = \" + str(FP))\n",
    "print(\"TN = \" + str(TN))\n",
    "print(\"FN = \" + str(FN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO Show that calculating accuracy using true/false positives/negatives will get the same answer\n",
    "\n",
    "TFAccuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "TFAccuracy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "TPR = TP / (TP + FN)\n",
    "TNR = TN / (TN + FP)\n",
    "print(TPR[0], TNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced error rate = [0.5]\n"
     ]
    }
   ],
   "source": [
    "BER = 1 - 1/2 * (TPR + TNR)\n",
    "print(\"Balanced error rate = \" + str(BER)) #Answer should be .5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Precision and Recall\n",
    "\n",
    "Precision and Recall are often used to rank performance of models. These can both be defined in terms of True Positives, False Positives, and False Negatives. (Though knowing any three of these true/false positives/negatives will give you the number of the remaining term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.75]), array([1.]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO Calculate Precision and recall in the terms defined in lecture.\n",
    "\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "\n",
    "precision, recall #Answers should be (.75, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1 score is just the average (precisely, the harmonic mean) of precision and recall. This is useful since it's easy to have either a good precision, or a good recall in isolation, but it's hard for both values to be high simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.85714286])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1 = 2 * (precision*recall) / (precision + recall)\n",
    "F1 #Answer should be roughly .857..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You're All Done!\n",
    "\n",
    "We have now introduced some basic ideas of classification and ranking. Next week you will use them on a dataset of reviews and see how these ideas can be applied to a proper dataset!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
